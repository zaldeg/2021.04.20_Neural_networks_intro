{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python395jvsc74a57bd09dc16db89c7b7868c1bda26003793495fc53e348f30248cc96f0b4588f8ccbba",
   "display_name": "Python 3.9.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "9dc16db89c7b7868c1bda26003793495fc53e348f30248cc96f0b4588f8ccbba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Практическое задание\n",
    "\n",
    "Вариант 1. (простой)\n",
    "\n",
    "- обучить сверточную нейронную сеть в стиле AlexNet (с падением размера ядра свертки и последовательностью блоков свертка-пулинг  (conv-pool)-(conv-pool)-...) на датасете fashion-mnist\n",
    "- оценить рост точности при увеличении ширины сети (больше ядер)\n",
    "- оценить рост точности при увеличении глубины сети (больше слоев)\n",
    "- сравнить с точностью полносвязной сети для этой выборки\n",
    "    </li>\n",
    "\n",
    "Вариант 2. (сложный)\n",
    "- реализовать нейронную сеть или стек из сверток (Concatenate) на сifar10.\n",
    "- оценить рост точности при увеличении ширины сети (больше ядер), больше нитей.\n",
    "- оценить рост точности при увеличении глубины сети (больше слоев)\n",
    "    </li>\n",
    "</ol>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, Input\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Lambda\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Resizing\n",
    "\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalization()\n",
    "normalizer.adapt(x_train)\n",
    "x_train = normalizer(x_train)\n",
    "x_test = normalizer(x_test)\n",
    "y_train = to_categorical(y_train, 10) \n",
    "y_test = to_categorical(y_test, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.reshape(x_train,(x_train.shape[0], 28,28,1))\n",
    "x_test = tf.reshape(x_test,(x_test.shape[0], 28,28,1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TensorShape([60000, 28, 28, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  fit_nn(x_train, y_train, hiden_shapes=[[128,64,32]], cv=3):\n",
    "    results = []\n",
    "    for shape in hiden_shapes:\n",
    "        cv_results =[]\n",
    "        for i in range(cv):\n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(layers.Conv2D(64, kernel_size=(7, 7), strides=4, activation='relu', input_shape=(28,28,1), padding=\"same\"))\n",
    "            model.add(layers.AveragePooling2D(pool_size=(2, 2), strides=2, padding='same'))\n",
    "\n",
    "            model.add(layers.Conv2D(64, kernel_size=(5, 5), strides=4, activation='relu', padding='same'))\n",
    "            model.add(layers.AveragePooling2D(pool_size=(2, 2), strides=2, padding='same'))\n",
    "\n",
    "\n",
    "            model.add(layers.Conv2D(64, kernel_size=(3, 3), strides=4, activation='relu', padding='same'))\n",
    "            model.add(layers.Conv2D(64, kernel_size=(3, 3), strides=4, activation='relu', padding='same'))\n",
    "            model.add(layers.Conv2D(32, kernel_size=(3, 3), strides=4, activation='relu', padding='same'))\n",
    "\n",
    "            model.add(layers.Flatten())\n",
    "\n",
    "            for i in range(len(shape)):\n",
    "                model.add(layers.Dense(shape[i], activation='relu'))\n",
    "        \n",
    "            model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "            model.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer='adam', metrics=[\"accuracy\"])\n",
    "            hist = model.fit(x=x_train,y=y_train, epochs=40, batch_size=1024, validation_data=(x_test, y_test), verbose=0)\n",
    "\n",
    "        cv_results.append(round(hist.history['val_accuracy'][-1], 4) * 100)\n",
    "        results.append((shape, np.mean(cv_results)))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_shape = [i for i in range(50, 400, 100)]\n",
    "hiden_shapes1 = [[i, i//2] for i in max_shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 18min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    " result1 = fit_nn(x_train, y_train, hiden_shapes=hiden_shapes1, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiden_shapes2 = [[280, 150], [270,  150, 80], [260, 150, 80, 50, 25], [230, 130, 110, 100 , 90, 80, 60, 40, 30, 20 ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 19min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result2 = fit_nn(x_train, y_train, hiden_shapes=hiden_shapes2, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[([50, 25], 88.28),\n",
       " ([150, 75], 88.57000000000001),\n",
       " ([250, 125], 88.58),\n",
       " ([350, 175], 88.85)]"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "result1"
   ]
  },
  {
   "source": [
    "Как мы видим очень простая сеть справляется практически так же хорошо как и очень сложная."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[([280, 150], 88.42),\n",
       " ([270, 150, 80], 89.11),\n",
       " ([260, 150, 80, 50, 25], 88.57000000000001),\n",
       " ([230, 130, 110, 100, 90, 80, 60, 40, 30, 20], 87.99)]"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "result2"
   ]
  },
  {
   "source": [
    "Тут тоже сети справляются примерно одинаково, но количество уровней при той же сложности ухудшает результат.\n",
    "Во втором домашнем задании лучший результат был accuracy 87,7%(правда это не кроссвалидированный результат, так что может в среднем она выдает меньшую точность)  хуже  этой сверточной сети, но не сильно.\n",
    "Честно говоря я не совсем понимаю как настраивать такие сети, ведь в них очень много параметров, и сети работают нестабильно. что бы сделать нормальный грид серч с кросс валидацией нужно тратить чудовещное количество времени."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}